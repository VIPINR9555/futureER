{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a958315-4d70-43bf-bfa4-b5ccb8f1bb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "                                            ## FEATURE ENGINEERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e821377f-3ff8-4780-ba07-d09088da54c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is a parameter?\n",
    "Ans A parameter is a variable that is used to define the behavior of a model or a function. In the context of machine learning,\n",
    "    parameters are the internal variables of a model that are learned during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133dc4ac-d592-427a-bc5b-737e849cfac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. What is correlation?\n",
    "Ans Correlation is a statistical measure that describes the relationship between two continuous variables. It measures how much\n",
    "    the variables tend to change together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d4ad53-5cb0-4770-82c0-3bf7ed57a921",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. What does negative correlation mean?\n",
    "Ans Negative correlation means that as one variable increases, the other variable tends to decrease. In other words, the variables\n",
    "    move in opposite directions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27aef72-1030-4d02-8459-a37763db5d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. Define Machine Learning. What are the main components in Machine Learning?\n",
    "Ans Machine learning is a subfield of artificial intelligence that involves training algorithms to learn from data and make\n",
    "    predictions or decisions.\n",
    "    The main components of machine learning are:\n",
    "\n",
    "    1. Data: The input data used to train the model.\n",
    "    2. Model: The algorithm or mathematical function that is trained on the data.\n",
    "    3. Parameters: The internal variables of the model that are learned during training.\n",
    "    4. Loss function: A mathematical function that measures the difference between the model's predictions and the actual outputs.\n",
    "    5. Optimization algorithm: An algorithm that adjusts the model's parameters to minimize the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5862bf-5b98-405c-b028-9e5c7b0acf05",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. How does loss value help in determining whether the model is good or not?\n",
    "Ans The loss value measures the difference between the model's predictions and the actual outputs. A lower loss value indicates\n",
    "    that the model is performing well, while a higher loss value indicates that the model needs improvement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2681c8d-989b-4d49-a7e8-2228636af556",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. What are continuous and categorical variables?\n",
    "Ans Continuous variables are numerical variables that can take on any value within a range, such as height or weight.\n",
    "    Categorical variables are variables that take on distinct categories or labels, such as gender or color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecf958b-1fa9-4e0b-bd79-6e2ebdd8244a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7. How do we handle categorical variables in Machine Learning? What are the common techniques?\n",
    "Ans Categorical variables need to be converted into numerical variables before they can be used in machine learning models. Common\n",
    "    techniques for handling categorical variables include:\n",
    "\n",
    "    1. One-hot encoding: Creating a new binary variable for each category.\n",
    "    2. Label encoding: Assigning a numerical value to each category.\n",
    "    3. Ordinal encoding: Assigning a numerical value to each category, where the values have a natural order.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c08815-fc0b-40f4-aceb-c392bad816ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q8. What do you mean by training and testing a dataset?\n",
    "Ans Training a dataset involves using the data to train a machine learning model. Testing a dataset involves evaluating the performance\n",
    "    of the trained model on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b523df4-18cf-4f10-9a08-14cd77d67649",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q9. What is sklearn.preprocessing?\n",
    "Ans sklearn.preprocessing is a module in scikit-learn that provides functions for preprocessing data, such as scaling, normalizing,\n",
    "    and encoding categorical variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a571311f-e2ea-40cd-814d-dbf0db62374b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q10. What is a Test set? How do we split data for model fitting (training and testing) in Python? How do you approach a Machine\n",
    "     Learning problem?\n",
    "Ans  A test set is a portion of the data that is used to evaluate the performance of a trained model.To split data for model fitting\n",
    "    in Python, you can use the train_test_split function from scikit-learn. \n",
    "    \n",
    "    Here's an example:\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    This code splits the data into training and testing sets, with 80% of the data used for training and 20% used for testing.\n",
    "    \n",
    "    Here's a step-by-step approach to tackling a Machine Learning problem:\n",
    "    \n",
    "    Step 1: Problem Formulation\n",
    "    1. Define the problem: Clearly articulate the problem you're trying to solve.\n",
    "    2. Identify the goal: Determine what you want to achieve with Machine Learning.\n",
    "    3. Gather context: Collect relevant information about the problem domain.\n",
    "    \n",
    "    Step 2: Data Collection\n",
    "    1. Identify data sources: Determine where to collect data from.\n",
    "    2. Collect and store data: Gather and store the data in a suitable format.\n",
    "    3. Document data: Record metadata and data descriptions.\n",
    "    \n",
    "    Step 3: Data Exploration\n",
    "    1. Visualize data: Use plots and charts to understand the data distribution.\n",
    "    2. Summary statistics: Calculate mean, median, mode, and standard deviation.\n",
    "    3. Correlation analysis: Examine relationships between variables.\n",
    "    \n",
    "    Step 4: Data Preprocessing\n",
    "    1. Handle missing values: Decide on a strategy for missing data (e.g., imputation, removal).\n",
    "    2. Data normalization: Scale/normalize data to prevent feature dominance.\n",
    "    3. Feature engineering: Create new features or transform existing ones.\n",
    "    \n",
    "    Step 5: Model Selection\n",
    "    1. Choose a model type: Select a suitable algorithm based on the problem (e.g., regression, classification, clustering).\n",
    "    2. Consider model complexity: Balance model simplicity with performance.\n",
    "    \n",
    "    Step 6: Model Training\n",
    "    1. Split data: Divide data into training, validation, and testing sets.\n",
    "    2. Train the model: Use the training data to train the selected model.\n",
    "    3. Tune hyperparameters: Adjust model parameters for optimal performance.\n",
    "    \n",
    "    Step 7: Model Evaluation\n",
    "    1. Evaluate metrics: Calculate relevant metrics (e.g., accuracy, precision, recall, F1-score).\n",
    "    2. Cross-validation: Assess model performance on unseen data.\n",
    "    3. Compare models: Evaluate multiple models and choose the best one.\n",
    "    \n",
    "    Step 8: Model Deployment\n",
    "    1. Deploy the model: Integrate the trained model into a production-ready system.\n",
    "    2. Monitor performance: Continuously evaluate the model's performance on new data.\n",
    "    3. Update and maintain: Refine the model as needed to ensure ongoing performance.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936a7fa4-b02f-46bd-ab28-a177dd7b3b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q11. Why do we have to perform EDA before fitting a model to the data?\n",
    "Ans  Performing Exploratory Data Analysis (EDA) before fitting a model helps to:\n",
    "    - Understand the distribution of variables\n",
    "    - Identify missing values and outliers\n",
    "    - Detect correlations and relationships between variables\n",
    "    - Inform model selection and feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee75ba5-1311-449f-b4a5-9f0f122c163b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q12. What is correlation?\n",
    "Ans  Correlation measures the strength and direction of the linear relationship between two continuous variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d879ae7-7720-43cd-9077-97ed2ba95095",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q13. What does negative correlation mean?\n",
    "Ans  Negative correlation means that as one variable increases, the other variable tends to decrease.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdac6775-f4f0-4893-9ec7-43f8d13f72e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q14. How can you find correlation between variables in Python?\n",
    "Ans  You can use the corr() function from pandas or the pearsonr() function from scipy.stats.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cea597-7efb-42cb-b158-d0db64f0461d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q15. What is causation? Explain difference between correlation and causation with an example.\n",
    "Ans  Causation implies a cause-and-effect relationship between variables. Correlation does not necessarily imply causation.\n",
    "\n",
    "    Example: \n",
    "    Ice cream sales are correlated with the number of people wearing shorts, but eating ice cream does not cause people to wear shorts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31172806-1d40-4c43-b95d-6ad48db05be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q16. What is an Optimizer? What are different types of optimizers? Explain each with an example.\n",
    "Ans  An optimizer is an algorithm used to adjust the parameters (weights and biases) of a machine learning model to minimize the loss\n",
    "     function and improve performance. It helps in faster convergence and better generalization of the model.\n",
    "\n",
    "     Types of Optimizers:\n",
    "    #Gradient Descent\n",
    "    #Stochastic Gradient Descent (SGD)\n",
    "    #Mini-batch Gradient Descent\n",
    "    #Momentum-based Gradient Descent\n",
    "    #AdaGrad (Adaptive Gradient Algorithm)\n",
    "    #RMSprop (Root Mean Square Propagation)\n",
    "    #Adam (Adaptive Moment Estimation)\n",
    "         \n",
    "    1. Gradient Descent\n",
    "       It is the most basic optimization algorithm that updates model parameters in the direction of the negative gradient of the loss\n",
    "       function.\n",
    "         \n",
    "       Example:\n",
    "       Used in linear regression and logistic regression for optimizing cost functions.\n",
    "\n",
    "       2. Stochastic Gradient Descent (SGD)\n",
    "          Instead of computing the gradient for the entire dataset, SGD updates weights using one random data point at a time, making \n",
    "          it faster but noisier. \n",
    "\n",
    "        Example:\n",
    "        Used in real-time applications like recommendation systems.\n",
    "\n",
    "       3. Mini-batch Gradient Descent\n",
    "          A compromise between batch and stochastic gradient descent, mini-batch GD updates weights using small batches of data.\n",
    "\n",
    "          Example:\n",
    "          Used in deep learning with TensorFlow/Keras for training large datasets efficiently.\n",
    "\n",
    "       4. Momentum-based Gradient Descent\n",
    "          It adds a moving average of past gradients to accelerate convergence and avoid local minima.\n",
    "\n",
    "          Example:\n",
    "          Helps in training deep neural networks by avoiding oscillations.\n",
    "    \n",
    "       5. AdaGrad (Adaptive Gradient Algorithm)\n",
    "          It adapts the learning rate for each parameter by scaling it based on past gradients.\n",
    "    \n",
    "          Example:\n",
    "          Used in NLP and sparse data problems.\n",
    "    \n",
    "       6. RMSprop (Root Mean Square Propagation)\n",
    "          It maintains a moving average of squared gradients and divides the learning rate by the square root of this average to\n",
    "          control step size.\n",
    "    \n",
    "          Example:\n",
    "          Used in recurrent neural networks (RNNs).\n",
    "\n",
    "       7. Adam (Adaptive Moment Estimation)\n",
    "          It combines momentum and RMSprop to adapt learning rates individually for each parameter.\n",
    "    \n",
    "          Example:\n",
    "          Used in almost all deep learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3850c6e3-4971-4b8a-b016-cc2e0b946937",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q17. What is sklearn.linear_model ?\n",
    "Ans  sklearn.linear_model is a module in scikit-learn that provides classes for linear regression models, including LinearRegression, \n",
    "     Ridge, Lasso, and ElasticNet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edd1ee8-a309-43b6-a016-7c31eb348c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q18. What does model.fit() do? What arguments must be given?\n",
    "Ans  model.fit() trains the model on the provided data.\n",
    "\n",
    "    Required arguments:    \n",
    "    - X: feature matrix\n",
    "    - y: target vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1c7f0e-8b57-4e78-80e1-54d11fa5abbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q19. What does model.predict() do? What arguments must be given?\n",
    "Ans  model.predict() generates predictions for the provided input data. \n",
    "     \n",
    "     Required argument:- X: feature matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddfbe31-8a26-4b85-a203-c618a3971030",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q20. What are continuous and categorical variables?\n",
    "Ans  Continuous variables are numerical variables that can take on any value within a range. Categorical variables are variables\n",
    "     that take on distinct categories or labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26cd512-f202-40c0-a3a3-cd829197b6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q21. What is feature scaling? How does it help in Machine Learning?\n",
    "Ans  Feature scaling transforms numerical variables to have similar scales, which helps to:\n",
    "        \n",
    "     - Improve model convergence\n",
    "     - Reduce feature dominance\n",
    "     - Enhance interpretability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572090c5-eecf-4091-a1df-63466b67d3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q22. How do we perform scaling in Python?\n",
    "Ans  You can use the StandardScaler or MinMaxScaler from scikit-learn.\n",
    "\n",
    "     ##StandardScaler\n",
    "     The StandardScaler scales the data to have a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "     from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "    # Create a sample dataset\n",
    "    data = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n",
    "    \n",
    "    # Create a StandardScaler object\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # Fit and transform the data\n",
    "    scaled_data = scaler.fit_transform(data)\n",
    "    \n",
    "    print(scaled_data)\n",
    "    \n",
    "    \n",
    "    ##MinMaxScaler\n",
    "    The MinMaxScaler scales the data to a specific range, usually between 0 and 1.\n",
    "    \n",
    "    \n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    \n",
    "    # Create a sample dataset\n",
    "    data = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n",
    "    \n",
    "    # Create a MinMaxScaler object\n",
    "    scaler = MinMaxScaler()\n",
    "    \n",
    "    # Fit and transform the data\n",
    "    scaled_data = scaler.fit_transform(data)\n",
    "    \n",
    "    print(scaled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be963f39-5b59-4fc8-a7fc-a437cac182cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q23. What is sklearn.preprocessing?\n",
    "Ans     sklearn.preprocessing is a module in scikit-learn that provides functions for preprocessing data, including scaling, normalization,\n",
    "     and encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7c4392-408e-4d55-ab4c-f72200f332df",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q24. How do we split data for model fitting (training and testing) in Python?\n",
    "Ans     In Python, you can split data for model fitting (training and testing) using the train_test_split function from the\n",
    "     sklearn.model_selection module.\n",
    "\n",
    "     ##Here's an example:\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Create a sample dataset\n",
    "    data = {'Feature1': [1, 2, 3, 4, 5],\n",
    "            'Feature2': [6, 7, 8, 9, 10],\n",
    "            'Target': [0, 0, 0, 1, 1]}\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Split the data into features (X) and target (y)\n",
    "    X = df[['Feature1', 'Feature2']]\n",
    "    y = df['Target']\n",
    "    \n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    print(\"Training data:\", X_train.shape, y_train.shape)\n",
    "    print(\"Testing data:\", X_test.shape, y_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc656eb7-24e3-467b-82ce-73a9249f4f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q25. Explain data encoding.\n",
    "Ans  Data encoding transforms categorical variables into numerical representations that can be processed by machine learning algorithms. \n",
    "     \n",
    "     Common encoding techniques include:\n",
    "    - Label encoding\n",
    "    - One-hot encoding\n",
    "    - Binary encoding\n",
    "    These techniques enable algorithms to handle categorical variables and improve model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc336fd-c18f-4e9a-aff0-dbb44eb98924",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
